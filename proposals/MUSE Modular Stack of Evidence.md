## Welcome To The Growth Pool Proposal Form

This form gathers the key details of your proposal to be shared to the community and provide the evaluation council key information to evaluate your proposal.

**Who's this Pool for?**

- Experienced builders with an idea addressing a critical infrastructure need

- Builders of existing public good tooling looking to become more interoperable

**Here are the step you will take:**

1. **Fill Out From** - Provide a detailed yet concise overview of your proposal.

2. **Community Feedback** - Address feedback and questions from the community and evaluators.

3. **Gardens Submission** - Once feedback is addressed, publish proposal on Gardens platform with the funding amount, title, problem being solved and Charmverse proposal link.

Once submitted to Gardens, the Evaluation Council will review your proposal and then support proposals on Gardens. You'll be able to see evaluation feedback on Charmverse and conviction growing on Gardens.

> _As your application moves through the evaluation flow you will receive notifications sent to your email used with Charmverse._
>
> _If you have questions or need support, [reach out on Telegram](https://t.me/+U4CtT8tOneZlZGRh)._ 

### What categories of public goods does your proposal address?

Impact Measurement, Governance/Coordination, Capital Allocation

### Problem Being Solved

The primary challenge MUSE addresses is the difficulty in applying Evidence-based Practice (EBP)—specifically, bridging the gap in evidence communication—within the digital public goods ecosystem.

- Concise Description: In public goods space, Retro Funding is popular because we believe that "it’s easier to agree on what has been useful than on what will be useful." In order to recognize "what has been useful", we try to evaluate impacts. But, it's hard to evaluate impacts because it is hard to create a causal model with using evidence. Actually, scientific evidence regarding intervention effectiveness is often fragmented, specialized, and difficult to utilize by practitioners (builders and funders) for decision-making. This lack of an accessible "Evidence Layer" hinders the ability of DAOs and Foundations to rationally allocate capital, leading to funding decisions on projects with unverified outcomes and risking fund depletion.

- Measurable Challenge: EBP requires defining a causal model (P stage) to link outputs to outcomes. Currently, the ecosystem is biased toward post-implementation evaluation (C stage). MUSE aims to lower barriers to EBP by making scientific knowledge readily available, enabling projects to articulate scientifically grounded logic models during the planning phase, thus improving funding efficiency and the predictability of project outcomes.

### Team

Shinya Mori - Co-founder / Project Manager

He conducts research in the field of digital public goods, with a particular focus on public goods funding mechanisms and grant program analysis. He contributes to MUSE by advancing research and dissemination of knowledge around the sustainability of DPGs and the crypto ecosystem. He has presented at international conferences such as Devcon SEA and co-hosted major global events including Funding the Commons Tokyo and Plurality Tokyo, promoting the understanding and growth of DPGs both in Japan and internationally. His published works include:

- A Retrospective Quantitative Review of Crypto Grants Programs: https://beaconlabs.io/reports/a-retrospective-quantitative-review-of-crypto-grants-programs/

- Positive Sum Design with Crypto: https://beaconlabs.io/reports/positive-sum-design-with-crypto/

- Instrumentalized Plurality: Dialectic of Enlightenment fixes Plurality: https://beaconlabs.io/reports/instrumentalized-plurality-dialectic-of-enlightenment-fixes-plurality/

He also gave talks at:

- Devcon SEA: https://youtu.be/YyoQSc4iDPk?si=gl0dQVi0NKKuQMbO

- Funding the Commons Tokyo: https://youtu.be/td3FsdAjypU?feature=shared

Naoki Akazawa - Co-founder / Data Scientist, Engineer

He leads MUSE’s technical development and plays a pivotal role in its evidence-based practice (EBP) and evaluation framework. As a Certified Evaluator of the Japan Evaluation Society, he provides expertise in logic model design, impact measurement, and evidence assessment, directly enhancing MUSE’s accountability, transparency, and efficiency. Technically, he has extensive experience in machine learning, blockchain, and system architecture. He has served as CTO at Fracton Ventures, where he led multiple development projects. His expertise enables MUSE’s “learning evidence” functionality—automatically updating evidence reliability—and supports the system’s open-source and open-data integration design.

Shu Tanaka - Co-founder / Engineer

He specializes in smart contract and protocol development. He builds core blockchain components of MUSE, ensuring technical robustness and on-chain interoperability. He has a proven record in the public goods and crypto space, having contributed to projects such as:

- ggframe (https://github.com/gg-frame/ggframe) - a donation support tool for Gitcoin Grants

- Eternal Commons (https://ethglobal.com/showcase/eternal-commons-dnt9j) – a tool for continuous OSS funding

- FluidFunding (https://ethglobal.com/showcase/prediction-based-quadratic-funding-protocol-vrasm) – a prediction-market-based quadratic funding protocol

He also contributed to Hypercerts, a standard for impact attribution. He has deep expertise in impact evaluation and has led initiatives like the Impact Evaluation Service (IES)—a blockchain-based incentive system for impact evaluation reports. He participated in the Impact Evaluator Research Retreat (IERR) and discussed how to evaluate impacts, providing MUSE with foundational knowledge for modeling evidence and understanding causal relationships between interventions and outcomes.

### Karma GAP Profile

https://gap.karmahq.xyz/project/beacon-labs

### Github Repo or Organization (If Multiple Repos)

https://github.com/beaconlabs-io/muse

### Project Tech Stack

- Core Stack: The MUSE platform leverages data science and data engineering techniques, including advanced computation like AI-driven evidence synthesis and potential use of machine learning for causal discovery/inference and automated curation.

- Interoperability Primitives:

  - Hypercerts: MUSE complements Hypercerts by providing a framework to create evidence-based logic models before issuance, strengthening the Hypercerts evaluation layer. MUSE will deliver connectors/importers for the Hypercerts impact data layer.

  - EAS (attestations): MUSE plans to store citations of evidence in EAS (or IPFS).

  - IPFS: Used for storage of evidence citations.

### Collaborations

We need more partnership with impact evaluators for increasing the number of evidence. For example, we have already connected with Open Source Observer (OSO), Eval.Science, GainForest, Hypercerts, Funding the Commons, and more.

Currently, MUSE has already integrated Hypercerts. We have to collaborate with impact evaluator like OSO and Eval.Science for create new evidence. And then, to utilize evidence in actual practice, we have to collaborate with grant programs, hackathon, and conference like Funding the Commons and FIL-RetroPGF.

### Deliverables

D1: Stable, production-grade MUSE Curation/Citation platform Acceptance criteria:

- Hardened MVP with production-level reliability, observability, and API stability

- Adoption toolkit (SDK/CLI, templates, tutorials, reference projects) publicly released

- Evidence-card schema (Interventions, Outcomes, Indicators) finalized and documented

Verification:

- Functional demo and public instance available at "https://muse.beaconlabs.io/"

- Open-source repository updated and tagged with release v1.0 on GitHub

D2: Utilize MUSE for a few practices Acceptance criteria:

- 3 practitioner organizations\*\* use MUSE evidence in real decisions (e.g., funding, program design, policy memos)

- Each decision memo or plan cites 1 MUSE evidence card (with Interventions/Outcomes/Indicators complete)

Verification:

- Public case studies showing end-to-end evidence-in-use for each decision.

- Hypercerts of used logic models created by MUSE.

### Budget

Total Requested Funding: $10,000 (for 6 month)

- MVP upgrade (Evidence Card functionality, Logic Model generation): $2,000

- Automated Evidence Curation using Deep Research: $2,000

- API Integration Development (connections with IPFS/EAS and other OSS tools): $2,000

- Server & Cloud Infrastructure (data storage and processing environment): $2,000

- Outreach, Documentation & Business Development Costs: $2,000

### GTM & Adoption Plan

The Go-to-Market (GTM) and adoption strategy for MUSE (Modular Stack of Evidence) is designed to establish it as the essential "Evidence Communication" layer for digital public goods by focusing on interoperability, providing easy-to-use tools, and leveraging existing community footprints.

### Initial Users/Integrators

MUSE targets stakeholders who produce, consume, or fund DPGs, primarily by integrating with existing open-source impact infrastructures.

**Primary Users (Consumers):** Funders and DPG Builders/Practitioners.

- For funders (DAOs, Foundations), MUSE enables data-driven decision-making and more effective capital allocation, shifting funding toward projects with verifiable pathways to impact.

- For builders, it accelerates project planning by enabling the creation of scientifically grounded logic models (the "Plan" stage of PDCA).

**Evidence Producers (Contributors):** Impact Evaluators and Analysts\*\*

MUSE seeks to onboard organizations that are already generating evidence, such as Open Source Observer (OSO) and Eval.Science.

**Initial Integrators (Protocols):** 

MUSE is designed to tightly complement and integrate with existing standards, specifically Hypercerts. Planned integrations also include other impact evaluation projects like KarmaGAP and Impact Garden.

### Promotion or Onboarding Adoption

The adoption approach focuses on lowering the technical and conceptual barriers to Evidence-based Practice (EBP).

- Interoperability and Tooling (Phase 1): The initial focus is on technical readiness and user enablement.

  - Shipping an Adoption Toolkit:

    - This toolkit will include a quick start guide, templates, CLI utilities, tutorials, and reference projects, making scientific knowledge readily available and usable by anyone.

  - Integration with Core Protocols:

    - We will deliver connectors/importers for the Hypercerts impact data layer to ensure immediate interoperability and strengthen Hypercerts’ evaluation layer.

  - Production Standards:

    - Reliability, performance, API stability, and comprehensive documentation will be elevated to production standards to support integration.

- Community and Incentivization (Phase 2): Leveraging the team’s existing community involvement is key to scaling adoption.

  - Community Footprint:

    - Beacon Labs has a deep footprint in the Funding the Commons (FtC) ecosystem, having co-organized Funding the Commons Tokyo and regularly presented at FtC events.

  - Active Promotion:

    - Adoption will be promoted by showcasing evidence-based practice projects using MUSE through grants programs, hackathons, residency programs, and meetups.

  - Incentives for Contribution:

    - To expand community participation, Phase 2 will launch an external evidence submission flow and integrate IES-based incentives (Impact Evaluation Service) to reward high-quality evidence contributions.

The project will measure impact and adoption primarily through the number of citations of MUSE. Growth in citations across grant applications, portfolio reviews, and post-hoc evaluations will signal that MUSE is effectively shaping evaluation narratives and investment memos.

### Post-Round Sustainability Strategy

MUSE’s long-term sustainability is built upon three pillars: its open-source nature, the creation of a decentralized contribution feedback loop, and active fundraising efforts.

- Open-Source Stewardship:

  - MUSE is being developed as open-source software. This ensures that technological development and knowledge utilization are widely shared, aligning with the goal of avoiding power concentration and promoting decentralization.

- Incentivized Feedback Loop (Phase 2 & 3):

  - Sustainability is secured by transitioning to a community-driven model where content creation and refinement are incentivized. The implementation of IES-based incentives and a structured external submission flow rewards contributors for generating high-quality evidence, ensuring the data corpus remains fresh and robust. Phase 3 focuses on scaling value through automated curation pipelines and synthesis tools (meta-analysis), which reduces manual effort and increases the throughput and decision-readiness of the platform, making it a valuable, self-improving knowledge ecosystem.

- Business Development and Fundraising:

  - The project budget includes allocation for Business Development Activities to support partnerships and fundraising, such as pitch deck creation and outreach to potential collaborators, ensuring financial support beyond the initial grant.

### Additional Links and Resources

\- GitHub: <https://github.com/beaconlabs-io/muse/tree/main/contents>

\- MVP: <https://muse.beaconlabs.io/>

\- Official portal page: <https://beaconlabs.io/>

\- Demo: <https://www.loom.com/share/cbee26ad837a4819b7260652699f4cc6?sid=a43b1428-b408-49ec-93e6-ccb589a57178>